
Copy
# README: Data Migration with Celery and PostgreSQL

## **Project Overview**
This project facilitates **asynchronous data migration** between two **PostgreSQL databases** using **Celery**. The system is designed for **parallel batch processing** and leverages **Redis** as a message broker. Multiple **Celery workers** handle data migration efficiently.

---

## **Setup and Running the Project**

### **1. Build and Start the Containers**
To build the Docker images, run:
```bash
docker-compose build
block ended and simple text started 
Then, start the services in detached mode:

docker-compose up -d
This will initialize the following services:

source_db (PostgreSQL - source database)

dest_db (PostgreSQL - destination database)

redis (Message Broker for Celery tasks)

worker (Celery worker instances for parallel processing)

scheduler (Task scheduler to manage batch execution)

2. Alternative Startup Method (Using PyCharm Services)
If using PyCharm, navigate to Services and manually start the scheduler and worker containers.

Manually Retry a Failed Batch
To retry failed batches, execute this command inside the worker container:

docker exec -it pythonproject-worker-1 python retry_retriable.py
